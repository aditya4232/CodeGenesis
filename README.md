# MemoRAG ULTRA

A production-grade, research-level AI knowledge engine that runs fully locally on your laptop.

## ğŸš€ Features

- **Multi-Modal RAG**: Process PDFs, images, audio, and scanned documents
- **Hybrid Retrieval**: Automatic Speed/Deep mode selection
- **5 AI Agents**: Planner, Retriever, Critic, Verifier, Teacher
- **Continual Learning**: Anti-forgetting memory system
- **3D Knowledge Graph**: Interactive visualization
- **Fact-Checking**: Automated verification with external KBs
- **Semantic Caching**: 10x speedup for similar queries
- **Research Metrics**: RAGAS evaluation + hallucination detection
- **100% Local**: No cloud, no costs, complete privacy

## ğŸ“‹ Requirements

- **OS**: Windows, macOS, or Linux
- **RAM**: 8-16 GB
- **Storage**: ~10 GB free space
- **Python**: 3.11+
- **Node.js**: 18+ (for frontend)
- **LM Studio**: Download from [lmstudio.ai](https://lmstudio.ai)

## ğŸ› ï¸ Quick Start

### 1. Install LM Studio

1. Download and install [LM Studio](https://lmstudio.ai)
2. Download a quantized model (recommended: `Phi-3-mini-4k-instruct-Q4_K_M` or `Mistral-7B-Instruct-v0.2-Q4_K_M`)
3. Start the local server (default: `http://localhost:1234`)

### 2. Backend Setup

```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
python -m spacy download en_core_web_sm
cp ../config/config.example.yaml ../config/config.yaml
# Edit config.yaml with your LM Studio settings
python -m app.main
```

Backend will run on `http://localhost:8000`

### 3. Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Frontend will run on `http://localhost:5173`

### 4. Ingest Example Data

```bash
cd scripts
python ingest_example_data.py
```

## ğŸ“š Documentation

- [Architecture](docs/architecture.md) - System design and components
- [Usage Guide](docs/usage_guide.md) - How to use MemoRAG ULTRA
- [API Reference](docs/api_reference.md) - API endpoints and examples
- [Standout Features](docs/standout_features.md) - What makes this special

## ğŸ¯ Usage

### Upload Documents

1. Open the web interface at `http://localhost:5173`
2. Navigate to "Documents" page
3. Drag and drop files (PDF, TXT, MD, images, audio)
4. Wait for processing to complete

### Ask Questions

1. Go to "Query" page
2. Type your question
3. Select mode (Auto/Speed/Deep) or let the system decide
4. View answer with confidence score and provenance

### Explore Knowledge

1. Visit "Timeline" to see knowledge evolution
2. Check "System Status" for metrics and health
3. View 3D knowledge graph visualization

## ğŸ—ï¸ Project Structure

```
GiblerXT/
â”œâ”€â”€ backend/           # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/      # API endpoints
â”‚   â”‚   â”œâ”€â”€ core/     # Core config and models
â”‚   â”‚   â”œâ”€â”€ rag/      # RAG engine
â”‚   â”‚   â”œâ”€â”€ agents/   # 5 AI agents
â”‚   â”‚   â”œâ”€â”€ memory/   # Continual learning
â”‚   â”‚   â”œâ”€â”€ intelligence/  # Active learning
â”‚   â”‚   â”œâ”€â”€ cache/    # Semantic caching
â”‚   â”‚   â”œâ”€â”€ factcheck/     # Fact verification
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ frontend/         # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ public/
â”œâ”€â”€ config/          # Configuration files
â”œâ”€â”€ docs/            # Documentation
â”œâ”€â”€ scripts/         # Utility scripts
â””â”€â”€ data/            # Data storage
```

## ğŸ”¬ Research Features

- **RAGAS Metrics**: Context relevance, answer faithfulness, recall
- **Hallucination Detection**: Automated fact verification
- **A/B Testing**: Compare retrieval strategies
- **Comparative Analysis**: Multi-document synthesis
- **Trend Analysis**: Knowledge evolution over time

## ğŸ¨ Advanced Features

- **Active Learning**: Query suggestions based on knowledge gaps
- **Semantic Caching**: 10x faster for similar queries
- **Multi-Modal**: Process text, images, audio, scanned docs
- **Self-Healing**: Automatic error detection and correction
- **Real-Time Monitoring**: Performance analytics and alerts

## ğŸ“Š Performance

- **Speed Mode**: <1.5s response time
- **Deep Mode**: <5s response time
- **Cached Queries**: <0.2s (10x faster)
- **Memory Usage**: ~5 GB RAM
- **Scalability**: 1000+ documents

## ğŸ¤ Contributing

This is a final-year CSE project. Contributions, suggestions, and feedback are welcome!

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

Built with:
- [LM Studio](https://lmstudio.ai) - Local LLM inference
- [FastAPI](https://fastapi.tiangolo.com/) - Backend framework
- [React](https://react.dev/) - Frontend framework
- [LangChain](https://langchain.com/) - LLM orchestration
- [FAISS](https://github.com/facebookresearch/faiss) - Vector search

## ğŸ“§ Contact

For questions or collaboration: [Your Email/GitHub]

---

**MemoRAG ULTRA** - Your local AI knowledge engine ğŸ§ âœ¨
